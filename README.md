# Baidu_BigData_Contest
This is a project about The 5th Baidu Big-Data Contest
# Baidu_BigData_Contest:"Urban Region Function Classification"

详情参考：[百度大数据竞赛][1]
# 任务描述
    建立模型，使用卫星图像数据和来自特定地理区域的用户行为数据对城市区域的功能进行分类。

市区功能表：

| 目录      | 区域功能     |  
| ---------- | :-----------:  |
| 001     | 居住区     |
| 002     | 学校     |
| 003     | 工业园     |
| 004     | 地铁站     |
| 005     | 机场     |
| 006     | 公园     |
| 007     | 购物区     |
| 008     | 行政区     |
| 009     | 医院     |


# 数据集

## 训练集
    1.train_image.tar.gz:
    包含40000张遥感影像，每张影像的像素为100*100，分辨率为1m。
    文件命名格式： AreaID_CategoryID.jpg。 
    例如000001_001.jpg，表示该图片为区域000001的遥感影像，该区域的功能类别为居住区
   
    2.train_visit.tar.gz：
    包含40000个文件，每个文件记录一个区域的用户的到访记录。
    文件命名格式: AreaID_CategoryID.txt。
    例如000001_001.txt，表示该文件记录区域为000001的用户到访行为，该区域的功能类别为居住区。
    文件格式为：USERID \t day_a&hour_x|hour_y|..., day_b&hour_x|hour_z|...
    例如：aff296a485010219 \t 20190129&21|22,20190218&19|20|21 
    表示用户aff296a485010219在2019年01月29日的21点、22点， 2019年02月18日的19点、20点和21点到访过该区域

## 测试集
    1.test_image.tar.gz：
    包含10000张遥感影像，每张影像的像素为100*100，分辨率为1m。
    文件命名格式： AreaID.jpg。 例如000001.jpg，表示该图片为区域000001的遥感影像
    
    2.test_visit.tar.gz：
    包含10000个文件，每个文件记录一个区域的用户的到访记录。
    文件命名格式: AreaID.txt。例如000001.txt，表示该文件记录区域为000001的用户到访行为。
    数据格式同train_visit.tar.gz
    
**选手需要提交的数据(result_data)**:

    选手需要对测试集中的每个区域的功能进行分类，提供的文件格式为： AreaID \t CategoryID

## 解决思路
  该题目既有图像数据，又有用户行为数据，关键点在于如何将二者结合起来，单单利用某一种数据可能难以达到很好的效果,如果拆开看两种数据.
  - 对于图像数据，可以利用深度学习方法进行分类，输入图像，输出9个类别。我选取了Densnet网络来进行该分类任务，源码见Densnet目录。
  但取得的效果很不理想，极度欠拟合，调参效果甚微，猜想可能主要问题出在数据上，九种区域的图像数据并不太具有很好的区分特征，肉眼都难以分辨，对于神经网络来讲可能很难抓住特征。
  
  - 对于用户行为数据，我对所有的.txt进行了处理，主要是利用特征工程+xgboost的思路。
  将时间处理成周内，周末两类（也尝试了分为周一到周天），将24小时的各个时刻分为4个阶段（或者24个小时直接用，不分段）。
  主要的特征就是访问次数，周内周末（或者周一到周天的访问频次）访问比，一天内各个时段的访问频率。
  送入xgboost经过调参，最终达到了0.52左右的测试准确率。无法通过参数继续提升的原因还是在于特征工程还不够合理，或许进一步进行特征选取会更加合理。
  通过计算各个特征的重要程度，发现总访问次数和周末、周日的频率特征很重要。
  
  - 进一步的思路：结合二者数据，利用深度学习处理行为数据或许不好操作，来得肯定不如xgboost高效。
  所以不妨提取图片数据的特征，将该特征也作为xgboost的输入，这样应该能起到结合二者特征的作用。
  因为图片是可以看做高维数据，不妨想办法降维，目前想到的办法就是t-sne，将图片数据降为为二维或者三维，当做xgboost的特征输入。
  方法肯定是可行可操作的，但是效果还不知道，有待进一步实验验证。

## 源码文件
    - get_data.ipynb：
    处理.txt文件数据，并以.csv文件存储，
    由于数据量巨大，如果就这样处理，在服务器上面估计得处理50h
    所以将40000条数据分成了8份，复制8个.py文件并行的处理，效率大大提高
    处理好的数据如下：
    
 !['!'](https://github.com/DLLXW/Baidu_BigData_Contest/blob/master/visit_data0.png)
    
   label为类别标签，num为总访问次数，weekend,weekday为周末周内访问次数，h1d-h4为00:00~24:00四个时段的访问次数。

    - Xgboost.ipynb：
      该文件就是处理数据并且送入xgboost训练的代码。里面有较为详细的注释，包含特征选取处理，编码，模型训练，参数调节等
    - Xgboost——unmerge.ipynb:
      这里面所用到的数据特征更为细致，没对周x和小时段进行划分，数据如下：data_raw.csv(40000条里面的一部分，并未全部处理)：
      
 !['!'](https://github.com/DLLXW/Baidu_BigData_Contest/blob/master/visit_data1.png)
      结果显示是比上面的xgboost.ipynb效果要好的，认为提取的特征还不如这个原始的数据来得效果好，
      这说明上面的时段分割还不够准确的描述特征，还需要进一步研究如何提取特征。


    - Densnet目录：
        就是利用图片数据来进行分类的源码。Densnet/readme.txt里面有简要介绍，具体还需要看源码，其实并不难理解。

# 下一步工作
    单独利用图片数据，神经网络模型没办法很好拟合。单独利用访问数据虽然能达到0.5以上的准确率，但还是不够，有待进一步的特征选择。
    - 思路一：
        同时前面也提到了，下一步主要是将图片数据降维，然后和行为数据一起进行xgboost分类。
    - 思路二：
        仍然是利用神经网络为工具，现在想办法将访问数据的特征和图片数据混合到一起，同时送入网络进行训练。
        这里主要需要关注的是如何将访问数据特征化，使其能和图片数据一起用作网络输入。

[1]: https://dianshi.baidu.com/competition/30/question
